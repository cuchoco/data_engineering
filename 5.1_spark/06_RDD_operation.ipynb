{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/23 23:59:54 WARN Utils: Your hostname, kuchoco-GP62-6QE resolves to a loopback address: 127.0.1.1; using 192.168.219.113 instead (on interface wlp2s0)\n",
      "22/07/23 23:59:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/kuchoco/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/07/23 23:59:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet('data/origin_green/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2019, 12, 19, 0, 52, 30), lpep_dropoff_datetime=datetime.datetime(2019, 12, 19, 0, 54, 39), store_and_fwd_flag='N', RatecodeID=1.0, PULocationID=264, DOLocationID=264, passenger_count=5.0, trip_distance=0.0, fare_amount=3.5, extra=0.5, mta_tax=0.5, tip_amount=0.01, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=4.81, payment_type=1.0, trip_type=1.0, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 9, 45, 58), lpep_dropoff_datetime=datetime.datetime(2020, 1, 1, 9, 56, 39), store_and_fwd_flag='N', RatecodeID=5.0, PULocationID=66, DOLocationID=65, passenger_count=2.0, trip_distance=1.28, fare_amount=20.0, extra=0.0, mta_tax=0.0, tip_amount=4.06, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=24.36, payment_type=1.0, trip_type=2.0, congestion_surcharge=0.0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.rdd.take(2) # RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.take(2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT\n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour,\n",
    "    PULocationID AS zone,\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    hour, zone \n",
    "ORDER BY\n",
    "    hour, zone\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD = df_green\\\n",
    "    .select('lpep_pickup_datetime', 'PULocationID' ,'total_amount')\\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 9, 45, 58), PULocationID=66, total_amount=24.36)]\n",
      "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 1, 9, 45, 58), PULocationID=66, total_amount=24.36)]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start = datetime(2020,1,1)\n",
    "\n",
    "print( RDD.filter(lambda row: row.lpep_pickup_datetime >= start).take(1) )\n",
    "\n",
    "####### Using Function ########\n",
    "\n",
    "def filter_outliers(row):\n",
    "    return row.lpep_pickup_datetime >= start\n",
    "\n",
    "print( RDD.filter(filter_outliers).take(1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_grouping(row):\n",
    "    hour = row.lpep_pickup_datetime.replace(minute=0, second=0, microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "    key = (hour, zone)\n",
    "\n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    value = (amount, count)\n",
    "\n",
    "    return (key, value)\n",
    "\n",
    "def calculate_revenue(left_value, right_value):\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "\n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "\n",
    "    return (output_amount, output_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((datetime.datetime(2020, 1, 1, 9, 0), 42), (799.7600000000002, 52)),\n",
       " ((datetime.datetime(2020, 1, 1, 9, 0), 255), (666.34, 28)),\n",
       " ((datetime.datetime(2020, 1, 1, 9, 0), 59), (50.900000000000006, 3)),\n",
       " ((datetime.datetime(2020, 1, 1, 9, 0), 36), (295.34000000000003, 11)),\n",
       " ((datetime.datetime(2020, 1, 1, 9, 0), 37), (175.67, 6)),\n",
       " ((datetime.datetime(2020, 1, 1, 9, 0), 146), (99.37, 6)),\n",
       " ((datetime.datetime(2020, 1, 1, 10, 0), 43), (21.76, 2)),\n",
       " ((datetime.datetime(2020, 1, 1, 10, 0), 82), (498.41000000000025, 39)),\n",
       " ((datetime.datetime(2020, 1, 1, 9, 0), 197), (72.01, 3)),\n",
       " ((datetime.datetime(2020, 1, 1, 9, 0), 177), (274.95, 10))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "RDD \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .take(10)\n",
    "\n",
    "\n",
    "## DateTime, LocationID, sum(amount), count 완성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "RevenuewRow = namedtuple('RevenueRow', ['hour', 'zone', 'revenue', 'count'])\n",
    "\n",
    "\n",
    "def unwrap(row):\n",
    "    return RevenuewRow(\n",
    "        hour = row[0][0],\n",
    "        zone = row[0][1],\n",
    "        revenue = row[1][0],\n",
    "        count = row[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result = RDD \\\n",
    "        .filter(filter_outliers) \\\n",
    "        .map(prepare_for_grouping) \\\n",
    "        .reduceByKey(calculate_revenue) \\\n",
    "        .map(unwrap) \\\n",
    "        .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hour,TimestampType,true),StructField(zone,LongType,true),StructField(revenue,DoubleType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "result_schema = types.StructType([\n",
    "    types.StructField('hour',types.TimestampType(), True),\n",
    "    types.StructField('zone',types.IntegerType(), True),\n",
    "    types.StructField('revenue',types.DoubleType(), True),\n",
    "    types.StructField('count',types.IntegerType(), True)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = RDD \\\n",
    "        .filter(filter_outliers) \\\n",
    "        .map(prepare_for_grouping) \\\n",
    "        .reduceByKey(calculate_revenue) \\\n",
    "        .map(unwrap) \\\n",
    "        .toDF(result_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+-----+\n",
      "|               hour|zone|           revenue|count|\n",
      "+-------------------+----+------------------+-----+\n",
      "|2020-01-01 09:00:00|  42| 799.7600000000002|   52|\n",
      "|2020-01-01 09:00:00| 255|            666.34|   28|\n",
      "|2020-01-01 09:00:00|  59|50.900000000000006|    3|\n",
      "|2020-01-01 09:00:00|  36|295.34000000000003|   11|\n",
      "|2020-01-01 09:00:00|  37|            175.67|    6|\n",
      "|2020-01-01 09:00:00| 146|             99.37|    6|\n",
      "|2020-01-01 10:00:00|  43|             21.76|    2|\n",
      "|2020-01-01 10:00:00|  82|498.41000000000025|   39|\n",
      "|2020-01-01 09:00:00| 197|             72.01|    3|\n",
      "|2020-01-01 09:00:00| 177|            274.95|   10|\n",
      "|2020-01-01 09:00:00| 196|             37.91|    2|\n",
      "|2020-01-01 09:00:00| 258|              45.3|    1|\n",
      "|2020-01-01 09:00:00|  29|              61.3|    1|\n",
      "|2020-01-01 08:00:00| 116|              6.81|    1|\n",
      "|2020-01-01 09:00:00|  85|             64.76|    4|\n",
      "|2020-01-01 10:00:00| 226|228.91000000000005|   14|\n",
      "|2020-01-01 11:00:00| 256|            606.72|   25|\n",
      "|2020-01-01 10:00:00| 243| 86.39000000000001|    5|\n",
      "|2020-01-01 10:00:00| 256|435.40000000000003|   18|\n",
      "|2020-01-01 10:00:00|  33|283.14000000000004|   10|\n",
      "+-------------------+----+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hour,TimestampType,true),StructField(zone,IntegerType,true),StructField(revenue,DoubleType,true),StructField(count,IntegerType,true)))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.write.parquet('tmp/green-revenue') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
